ğŸ“š RAG Chatbot with Gemini, Chroma & Streamlit

A Retrieval-Augmented Generation (RAG) chatbot that allows users to upload documents and ask questions grounded in their own knowledge base.

Built with:

Google Gemini (LLM)

HuggingFace embeddings

ChromaDB (local vector store)

Streamlit frontend

This project demonstrates an end-to-end RAG pipeline, from document ingestion to a production-style UI.

ğŸš€ Features

Upload documents (.txt, .md, .pdf, .docx)

Automatic document ingestion & chunking

Local vector search using embeddings

Context-grounded LLM responses (RAG)

Source attribution for answers

Chat-style interface with conversation history

Clear conversation button

Fully local & free-ish setup

ğŸ§  Architecture Overview
User Question
     â†“
Embedding (HuggingFace)
     â†“
Chroma Vector Search
     â†“
Relevant Chunks
     â†“
Prompt + Context
     â†“
Gemini LLM
     â†“
Answer + Sources

ğŸ—‚ï¸ Project Structure
rag-chatbot-gemini/
â”œâ”€â”€ app.py                # Streamlit UI
â”œâ”€â”€ llm_client.py         # Gemini LLM client
â”œâ”€â”€ vector_store.py       # Chroma + embeddings
â”œâ”€â”€ ingest.py             # Document ingestion
â”œâ”€â”€ rag_pipeline.py       # Retrieval + prompt logic
â”œâ”€â”€ docs/                 # Uploaded documents
â”œâ”€â”€ chroma_db/            # Vector DB (auto-created)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone the repo
git clone https://github.com/YOUR_USERNAME/rag-chatbot-gemini.git
cd rag-chatbot-gemini

2ï¸âƒ£ Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # macOS / Linux
venv\Scripts\activate     # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Set up environment variables
cp .env.example .env


Edit .env and add your Gemini API key:

GEMINI_API_KEY=your_api_key_here

ğŸ“¥ Ingest Documents

Place documents inside the docs/ folder or upload them via the UI.

Supported formats:

.txt

.md

.pdf

.docx

To ingest manually:

python ingest.py

ğŸ’¬ Run the Chatbot
streamlit run app.py


Then open:

http://localhost:8501

ğŸ“Œ Example Questions

What is Retrieval-Augmented Generation?

Summarize the uploaded document

What topics are covered in the PDF I uploaded?

Each answer includes source snippets used by the model.

ğŸ”’ Notes on Privacy & Cost

Embeddings are computed locally

Vector database runs locally

Only LLM calls go to Gemini

No paid services required (within free API limits)